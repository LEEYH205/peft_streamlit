import matplotlib.pyplot as plt
import numpy as np
import streamlit as st

from peft_utils.data import load_tiny_instruct
from peft_utils.model import DEFAULT_MODEL_ID, load_base_model
from peft_utils.train import train_once
from peft_utils.viz import create_comparison_chart, setup_korean_font

# í•œê¸€ í°íŠ¸ ì„¤ì •
setup_korean_font()

st.set_page_config(
    page_title="QLoRA - 4-bit ì–‘ìí™” + LoRA", page_icon="ğŸ§±", layout="wide"
)

st.title("ğŸ§± QLoRA â€” 4-bit ì–‘ìí™” + LoRA (Quantized LoRA)")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    model_id = st.text_input("Base model", value=DEFAULT_MODEL_ID, help="ê¸°ë³¸ ëª¨ë¸ ID")
    r = st.slider("LoRA rank (r)", 1, 64, 8, step=1, help="ì €ë­í¬ í–‰ë ¬ì˜ ì°¨ì›")
    alpha = st.slider("lora_alpha", 1, 128, 16, step=1, help="LoRA ìŠ¤ì¼€ì¼ë§ íŒ©í„°")
    dropout = st.slider("lora_dropout", 0.0, 0.5, 0.05, step=0.01, help="ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨")
    target_modules = st.text_input(
        "target_modules (ì‰¼í‘œ êµ¬ë¶„)", value="c_attn,c_proj", help="LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆë“¤"
    )
    epochs = st.number_input("epochs", 1, 10, 1, help="í•™ìŠµ ì—í¬í¬ ìˆ˜")
    lr = st.number_input(
        "learning_rate", 1e-6, 5e-3, 5e-4, step=1e-6, format="%.6f", help="í•™ìŠµë¥ "
    )

# ë©”ì¸ ì½˜í…ì¸ 
col1, col2 = st.columns([2, 1])

with col1:
    st.markdown(
        """
    ## ğŸ¯ QLoRAë€ ë¬´ì—‡ì¼ê¹Œìš”?

    **QLoRA**ëŠ” **Q**uantized + **LoRA**ì˜ ì¤„ì„ë§ì…ë‹ˆë‹¤.

    ### ğŸ§© ê¸°ì¡´ LoRAì˜ í•œê³„
    - **ë©”ëª¨ë¦¬ ë¶€ì¡±**: í° ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
    - **í•˜ë“œì›¨ì–´ ìš”êµ¬**: ê³ ì‚¬ì–‘ GPUê°€ í•„ìš”
    - **ë¹„ìš©**: ë¹„ì‹¼ GPU ì‚¬ìš© í•„ìš”

    ### âœ¨ QLoRAì˜ í•´ê²°ì±…
    - **4-bit ì–‘ìí™”**: ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ë¡œ ì••ì¶•í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    - **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ê¸°ì¡´ ëŒ€ë¹„ 8ë°° ë©”ëª¨ë¦¬ ì ˆì•½
    - **ì„±ëŠ¥ ìœ ì§€**: ì–‘ìí™”í•´ë„ ì„±ëŠ¥ ì €í•˜ ìµœì†Œí™”
    """
    )

with col2:
    # QLoRA êµ¬ì¡° ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(6, 4))

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    methods = ["Full Model", "LoRA", "QLoRA"]
    memory_usage = [100, 20, 12.5]  # ìƒëŒ€ì  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

    bars = ax.bar(methods, memory_usage, color=["red", "orange", "green"], alpha=0.7)
    ax.set_ylabel("ìƒëŒ€ì  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (%)")
    ax.set_title("QLoRA: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±")

    # ê°’ í‘œì‹œ
    for bar, usage in zip(bars, memory_usage):
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 1,
            f"{usage}%",
            ha="center",
            va="bottom",
        )

    plt.xticks(rotation=45)
    plt.tight_layout()

    st.pyplot(fig)

# ìƒì„¸ ì„¤ëª…
st.markdown("---")
st.markdown(
    """
## ğŸ”¬ QLoRAì˜ ì‘ë™ ì›ë¦¬

### ğŸ“Š ì–‘ìí™” ê³¼ì •
```
1. ì›ë³¸ ê°€ì¤‘ì¹˜ (32-bit float)
2. 4-bit ì–‘ìí™” (ì •ë°€ë„ ì†ì‹¤)
3. LoRA ì–´ëŒ‘í„° ì¶”ê°€ (í•™ìŠµ ê°€ëŠ¥)
4. ì¶”ë¡  ì‹œ ì–‘ìí™” í•´ì œ + LoRA ì ìš©
```

### ğŸ¨ ì‹œê°ì  ë¹„êµ
"""
)

# ì–‘ìí™” ê³¼ì • ì‹œê°í™”
col1, col2 = st.columns(2)

with col1:
    st.markdown("#### ğŸ”´ ì¼ë°˜ LoRA")
    fig, ax = plt.subplots(figsize=(5, 4))

    # ì¼ë°˜ LoRA êµ¬ì¡°
    ax.text(
        0.5,
        0.9,
        "ì›ë³¸ ê°€ì¤‘ì¹˜ (32-bit)",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"),
    )
    ax.arrow(0.5, 0.8, 0, -0.2, head_width=0.05, head_length=0.05, fc="red", ec="red")
    ax.text(
        0.5,
        0.6,
        "LoRA ì–´ëŒ‘í„°",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"),
    )
    ax.arrow(0.5, 0.5, 0, -0.2, head_width=0.05, head_length=0.05, fc="red", ec="red")
    ax.text(
        0.5,
        0.3,
        "ë©”ëª¨ë¦¬: 100%",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"),
    )

    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.set_title("ì¼ë°˜ LoRA")
    ax.axis("off")

    st.pyplot(fig)

with col2:
    st.markdown("#### ğŸŸ¢ QLoRA")
    fig, ax = plt.subplots(figsize=(5, 4))

    # QLoRA êµ¬ì¡°
    ax.text(
        0.5,
        0.9,
        "ì›ë³¸ ê°€ì¤‘ì¹˜ (32-bit)",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"),
    )
    ax.arrow(
        0.5, 0.8, 0, -0.2, head_width=0.05, head_length=0.05, fc="orange", ec="orange"
    )
    ax.text(
        0.5,
        0.6,
        "4-bit ì–‘ìí™”",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"),
    )
    ax.arrow(
        0.5, 0.5, 0, -0.2, head_width=0.05, head_length=0.05, fc="orange", ec="orange"
    )
    ax.text(
        0.5,
        0.3,
        "LoRA ì–´ëŒ‘í„°",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"),
    )
    ax.arrow(
        0.5, 0.2, 0, -0.2, head_width=0.05, head_length=0.05, fc="green", ec="green"
    )
    ax.text(
        0.5,
        0.0,
        "ë©”ëª¨ë¦¬: 12.5%",
        ha="center",
        va="center",
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"),
    )

    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.set_title("QLoRA")
    ax.axis("off")

    st.pyplot(fig)

# ì–‘ìí™” ìˆ˜ì¤€ë³„ ë¹„êµ
st.markdown("---")
st.markdown("## ğŸ’¾ ì–‘ìí™” ìˆ˜ì¤€ë³„ ë¹„êµ")

col1, col2 = st.columns(2)

with col1:
    st.markdown("### ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ")

    # ì–‘ìí™” ìˆ˜ì¤€ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    quantization_levels = [
        "32-bit (FP32)",
        "16-bit (FP16)",
        "8-bit (INT8)",
        "4-bit (INT4)",
    ]
    memory_usage = [100, 50, 25, 12.5]

    fig = create_comparison_chart(
        quantization_levels,
        memory_usage,
        "ì–‘ìí™” ìˆ˜ì¤€ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰",
        "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (%)",
    )
    st.pyplot(fig)

with col2:
    st.markdown("### âš¡ ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„")

    # ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„
    fig, ax = plt.subplots(figsize=(6, 4))

    x = memory_usage
    y = [100, 98, 95, 90]  # ìƒëŒ€ì  ì„±ëŠ¥

    ax.plot(x, y, "bo-", linewidth=2, markersize=8)
    ax.set_xlabel("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (%)")
    ax.set_ylabel("ìƒëŒ€ì  ì„±ëŠ¥ (%)")
    ax.set_title("ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„")
    ax.grid(True, alpha=0.3)

    # ì ì— ë¼ë²¨ ì¶”ê°€
    for i, (mem, perf) in enumerate(zip(x, y)):
        ax.annotate(
            f"{quantization_levels[i].split()[0]}",
            (mem, perf),
            textcoords="offset points",
            xytext=(0, 10),
            ha="center",
        )

    plt.tight_layout()
    st.pyplot(fig)

# ì¥ë‹¨ì 
st.markdown("---")
col1, col2 = st.columns(2)

with col1:
    st.markdown("### âœ… QLoRAì˜ ì¥ì ")
    st.markdown(
        """
    - ğŸ’¾ **ë©”ëª¨ë¦¬ ì ˆì•½**: 8ë°° ë©”ëª¨ë¦¬ ì ˆì•½
    - ğŸš€ **ë¹ ë¥¸ í•™ìŠµ**: í° ëª¨ë¸ë„ í•™ìŠµ ê°€ëŠ¥
    - ğŸ’° **ë¹„ìš© ì ˆì•½**: ì €ì‚¬ì–‘ GPUë¡œë„ í•™ìŠµ
    - ğŸ“± **ì ‘ê·¼ì„±**: ë” ë§ì€ ì‚¬ëŒì´ ì‚¬ìš© ê°€ëŠ¥
    - ğŸ”§ **í˜¸í™˜ì„±**: ê¸°ì¡´ LoRAì™€ ë™ì¼í•œ ì‚¬ìš©ë²•
    """
    )

with col2:
    st.markdown("### âš ï¸ QLoRAì˜ ë‹¨ì ")
    st.markdown(
        """
    - ğŸ¯ **ì •ë°€ë„ ì†ì‹¤**: 4-bit ì–‘ìí™”ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤
    - ğŸ§® **ì¶”ë¡  ì˜¤ë²„í—¤ë“œ**: ì–‘ìí™” í•´ì œ ê³¼ì • í•„ìš”
    - ğŸ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ì–‘ìí™” ì„¤ì • ì¶”ê°€ í•„ìš”
    - ğŸ“Š **ì•ˆì •ì„±**: ê·¹ë‹¨ì  ì–‘ìí™”ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„±
    """
    )

# ì‹¤ìŠµ ì„¹ì…˜
st.markdown("---")
st.markdown("## ğŸš€ QLoRA ì‹¤ìŠµí•˜ê¸°")

st.info(
    """
ğŸ’¡ **ì‹¤ìŠµ íŒ**:
- **4-bit ì–‘ìí™”**: ë©”ëª¨ë¦¬ ì ˆì•½ì´ ìµœìš°ì„ ì¼ ë•Œ ì‚¬ìš©
- **r (rank)**: 8-32ê°€ ì¢‹ì€ ì‹œì‘ì 
- **alpha**: ë³´í†µ rì˜ 2ë°°ë¡œ ì„¤ì •
- **ì£¼ì˜**: Mac í™˜ê²½ì—ì„œëŠ” 4-bit ì–‘ìí™”ê°€ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
"""
)

if st.button("ğŸš€ QLoRA ë°ëª¨ í•™ìŠµ ì‹¤í–‰", type="primary"):
    with st.spinner("QLoRA ëª¨ë¸ ì¤€ë¹„ ì¤‘..."):
        base, tok, quant_ok = load_base_model(model_id, four_bit=True)

        if not quant_ok:
            st.warning(
                "âš ï¸ ì´ í™˜ê²½ì—ì„œëŠ” 4-bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¼ë°˜ LoRAë¡œ í´ë°±í•©ë‹ˆë‹¤."
            )
            st.info(
                "ğŸ’¡ Mac í™˜ê²½ì—ì„œëŠ” 4-bit ì–‘ìí™”ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  ì¼ë°˜ LoRAë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
        else:
            st.success("âœ… 4-bit ì–‘ìí™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")

        train_ds, eval_ds = load_tiny_instruct()
        tm = [s.strip() for s in target_modules.split(",") if s.strip()]

        # í•™ìŠµ ì§„í–‰ë¥  í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i in range(4):
            if i == 0:
                status_text.text("ë‹¨ê³„ 1/4: ëª¨ë¸ ì¤€ë¹„")
            elif i == 1:
                status_text.text("ë‹¨ê³„ 2/4: ì–‘ìí™” ì ìš©")
            elif i == 2:
                status_text.text("ë‹¨ê³„ 3/4: LoRA ì–´ëŒ‘í„° êµ¬ì„±")
            else:
                status_text.text("ë‹¨ê³„ 4/4: í•™ìŠµ ì‹œì‘")
            progress_bar.progress((i + 1) * 0.25)
            import time

            time.sleep(0.5)

        # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
        try:
            metrics = train_once(
                base,
                tok,
                train_ds,
                eval_ds,
                method="lora",
                r=r,
                alpha=alpha,
                dropout=dropout,
                target_modules=tm,
                epochs=epochs,
                lr=lr,
                four_bit=quant_ok,
            )

            st.success("ğŸ‰ QLoRA í•™ìŠµ ì™„ë£Œ!")

            # ê²°ê³¼ ì‹œê°í™”
            col1, col2 = st.columns(2)

            with col1:
                st.metric("í•™ìŠµ ì†ì‹¤", f"{metrics.get('train_loss', 'N/A'):.4f}")
                st.metric("í‰ê°€ ì†ì‹¤", f"{metrics.get('eval_loss', 'N/A'):.4f}")

            with col2:
                st.metric("í•™ìŠµ ì‹œê°„", f"{metrics.get('train_runtime', 'N/A'):.2f}ì´ˆ")
                st.metric(
                    "ìƒ˜í”Œ/ì´ˆ", f"{metrics.get('train_samples_per_second', 'N/A'):.2f}"
                )

            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í‘œì‹œ
            st.markdown("### ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±")

            if quant_ok:
                efficiency = "4-bit ì–‘ìí™” ì ìš©ë¨ (ë©”ëª¨ë¦¬ 87.5% ì ˆì•½)"
                color = "green"
            else:
                efficiency = "ì¼ë°˜ LoRA (4-bit ì–‘ìí™” ë¯¸ì§€ì›)"
                color = "orange"

            st.info(f"**ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: {efficiency}")

        except Exception as e:
            st.error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# Mac í™˜ê²½ì—ì„œì˜ 4-bit ì–‘ìí™” ì œí•œì‚¬í•­
st.markdown("---")
st.markdown("## ğŸ Mac í™˜ê²½ì—ì„œì˜ 4-bit ì–‘ìí™” ì œí•œì‚¬í•­")

st.warning(
    """
âš ï¸ **ì¤‘ìš”**: Mac í™˜ê²½ì—ì„œëŠ” 4-bit ì–‘ìí™”ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì´ëŠ” í•˜ë“œì›¨ì–´ì™€ ì†Œí”„íŠ¸ì›¨ì–´ì˜ ì œì•½ì‚¬í•­ ë•Œë¬¸ì…ë‹ˆë‹¤.
"""
)

with st.expander("ğŸ” Macì—ì„œ 4-bit ì–‘ìí™”ê°€ ì•ˆ ë˜ëŠ” ì´ìœ "):
    st.markdown(
        """
    ### ğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ì œì•½
    - **Apple Silicon (M1/M2/M3)**: CUDA ì§€ì› ì•ˆí•¨
    - **Intel Mac**: GPU ë©”ëª¨ë¦¬ ì œí•œ
    - **Metal Performance Shaders (MPS)**: ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì§€ì›

    ### ğŸ“¦ ì†Œí”„íŠ¸ì›¨ì–´ ì œì•½
    - **bitsandbytes**: CUDA ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
    - **GPTQ/AWQ**: NVIDIA GPU ìµœì í™”
    - **PEFT ì–‘ìí™”**: CUDA ê¸°ë°˜ êµ¬í˜„

    ### ğŸ”§ ëŒ€ì•ˆ ë°©ë²•
    1. **ì¼ë°˜ LoRA**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì€ ë–¨ì–´ì§€ì§€ë§Œ ì•ˆì •ì 
    2. **IAÂ³**: ê·¹í•œì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±
    3. **Prompt Tuning**: ìµœì†Œí•œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©
    4. **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤**: GPU ì„œë²„ í™œìš©
    """
    )

with st.expander("ğŸ’» í™˜ê²½ë³„ ì–‘ìí™” ì§€ì› í˜„í™©"):
    st.markdown(
        """
    | í™˜ê²½ | 4-bit ì–‘ìí™” | 8-bit ì–‘ìí™” | 16-bit ì–‘ìí™” | ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± |
    |------|--------------|--------------|---------------|---------------|
    | **NVIDIA GPU** | âœ… ì§€ì› | âœ… ì§€ì› | âœ… ì§€ì› | ë§¤ìš° ë†’ìŒ |
    | **Apple Silicon** | âŒ ë¯¸ì§€ì› | âŒ ë¯¸ì§€ì› | âœ… ì§€ì› | ì¤‘ê°„ |
    | **Intel CPU** | âŒ ë¯¸ì§€ì› | âŒ ë¯¸ì§€ì› | âœ… ì§€ì› | ë‚®ìŒ |
    | **AMD GPU** | âš ï¸ ì œí•œì  | âš ï¸ ì œí•œì  | âœ… ì§€ì› | ì¤‘ê°„ |

    **ê²°ë¡ **: Mac í™˜ê²½ì—ì„œëŠ” ì¼ë°˜ LoRAë‚˜ IAÂ³ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìµœì„ ì…ë‹ˆë‹¤.
    """
    )

with st.expander("ğŸš€ Macì—ì„œ PEFT ìµœì í™” íŒ"):
    st.markdown(
        """
    ### ğŸ“± Apple Silicon ìµœì í™”
    - **MPS ê°€ì†**: Metal Performance Shaders í™œìš©
    - **ë©”ëª¨ë¦¬ ê´€ë¦¬**: íš¨ìœ¨ì ì¸ ë°°ì¹˜ í¬ê¸° ì„¤ì •
    - **ëª¨ë¸ í¬ê¸°**: ì ì ˆí•œ ëª¨ë¸ ì„ íƒ

    ### ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    - **LoRA rank**: 8-16 ë²”ìœ„ì—ì„œ ì‹¤í—˜
    - **ë°°ì¹˜ í¬ê¸°**: ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
    - **í•™ìŠµë¥ **: ì•ˆì •ì ì¸ ë²”ìœ„ì—ì„œ ì„¤ì •

    ### ğŸ’¾ ë©”ëª¨ë¦¬ ì ˆì•½ ì „ëµ
    - **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì ˆì•½
    - **Mixed Precision**: 16-bit í•™ìŠµ í™œìš©
    - **ëª¨ë“ˆë³„ ì ìš©**: í•„ìš”í•œ ë ˆì´ì–´ë§Œ ì„ íƒ
    """
    )

# ì¶”ê°€ ì •ë³´
st.markdown("---")
st.markdown("## ğŸ“š ë” ì•Œì•„ë³´ê¸°")

with st.expander("ğŸ” QLoRA ë…¼ë¬¸ ì •ë³´"):
    st.markdown(
        """
    **ë…¼ë¬¸**: "QLoRA: Efficient Finetuning of Quantized LLMs" (2023)
    **ì €ì**: Tim Dettmers, et al.
    **í•µì‹¬ ì•„ì´ë””ì–´**: 4-bit ì–‘ìí™”ì™€ LoRAë¥¼ ê²°í•©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¯¸ì„¸ì¡°ì •
    """
    )

with st.expander("ğŸ’¡ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€"):
    st.markdown(
        """
    - **ëŒ€ê·œëª¨ ëª¨ë¸**: 7B, 13B, 70B ëª¨ë¸ í•™ìŠµ
    - **ë¦¬ì†ŒìŠ¤ ì œì•½ í™˜ê²½**: ê°œì¸ ê°œë°œì, ì—°êµ¬ì
    - **ë¹„ìš© íš¨ìœ¨ì  í•™ìŠµ**: í´ë¼ìš°ë“œ ë¹„ìš© ì ˆì•½
    - **ì‹¤í—˜ì  ì—°êµ¬**: ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸
    """
    )

with st.expander("âš¡ ì„±ëŠ¥ ë¹„êµ"):
    st.markdown(
        """
    | ë°©ë²• | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ì„±ëŠ¥ | í•™ìŠµ ì†ë„ | í•˜ë“œì›¨ì–´ ìš”êµ¬ |
    |------|---------------|------|-----------|---------------|
    | Full Fine-tuning | 100% | ë†’ìŒ | ëŠë¦¼ | ê³ ì‚¬ì–‘ GPU |
    | LoRA | 20% | ë†’ìŒ | ë¹ ë¦„ | ì¤‘ê°„ GPU |
    | **QLoRA** | **12.5%** | **ë†’ìŒ** | **ë¹ ë¦„** | **ì €ì‚¬ì–‘ GPU** |
    """
    )

with st.expander("ğŸ”§ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­"):
    st.markdown(
        """
    - **ì–‘ìí™”**: GPTQ, AWQ ë“±ì˜ 4-bit ì–‘ìí™” ê¸°ë²•
    - **LoRA**: ì €ë­í¬ ì–´ëŒ‘í„°ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶”ê°€
    - **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    - **ì•ˆì •í™”**: ì–‘ìí™” ë…¸ì´ì¦ˆë¥¼ LoRAë¡œ ë³´ìƒ
    """
    )
